import torch
import numpy as np
import cv2
import matplotlib.pyplot as plt
import os
from PIL import Image
import warnings

warnings.filterwarnings('ignore')


class YOLOv8DDAWAHeatmapVisualizer:
    def __init__(self, model_path, img_size=640):
        self.model_path = model_path
        self.img_size = img_size
        self.model = None
        self.feature_maps = {}
        self.attention_maps = {}
        self.hooks = []
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    def load_model(self):
        try:
            # åŠ è½½æ£€æŸ¥ç‚¹æ–‡ä»¶
            checkpoint = torch.load(self.model_path, map_location=self.device)

            # æ£€æŸ¥æ˜¯å¦æ˜¯Ultralyticsæ ¼å¼çš„æ£€æŸ¥ç‚¹
            if 'model' in checkpoint:
                # è¿™æ˜¯Ultralyticsæ ¼å¼çš„æ£€æŸ¥ç‚¹
                self.model = checkpoint['model']
                print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹ (æ£€æŸ¥ç‚¹æ ¼å¼)")
                print(f"è®­ç»ƒè½®æ¬¡: {checkpoint.get('epoch', 'N/A')}")
                print(f"æœ€ä½³fitness: {checkpoint.get('best_fitness', 'N/A')}")
                print(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in self.model.parameters()):,}")

                # ç¡®ä¿æ¨¡å‹åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                self.model = self.model.to(self.device)

                # ç»Ÿä¸€æ•°æ®ç±»å‹
                if next(self.model.parameters()).dtype == torch.float16:
                    print("âš ï¸ æ£€æµ‹åˆ°åŠç²¾åº¦æ¨¡å‹ï¼Œè½¬æ¢ä¸ºå•ç²¾åº¦")
                    self.model = self.model.float()

                return True
            elif hasattr(checkpoint, 'model'):
                # ç›´æ¥æ¨¡å‹å¯¹è±¡
                self.model = checkpoint.model
                print(f"âœ… æˆåŠŸåŠ è½½YOLOv8-DDAWAæ¨¡å‹ (ç›´æ¥æ¨¡å‹)")
                self.model = self.model.to(self.device).float()
                return True
            else:
                # å°è¯•ä½œä¸ºæƒé‡æ–‡ä»¶å¤„ç†
                print("âš ï¸ å°è¯•ä½œä¸ºæƒé‡æ–‡ä»¶å¤„ç†")
                self.model = checkpoint
                self.model = self.model.to(self.device).float()
                print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹æƒé‡")
                return True

        except Exception as e:
            print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
            return False

    def register_hooks(self):
        """æ³¨å†Œé’©å­æ¥æ•è·ç‰¹å¾å›¾å’Œæ³¨æ„åŠ›å›¾"""
        if self.model is None:
            return False

        # æŸ¥æ‰¾æ‰€æœ‰ConcatWithDDAWAæ¨¡å—
        ddawa_count = 0
        for name, module in self.model.named_modules():
            if 'ConcatWithDDAWA' in str(type(module)):
                # ä¸ºæ¯ä¸ªDDAWAæ¨¡å—æ³¨å†Œé’©å­
                hook = module.register_forward_hook(self._get_ddawa_hook(name))
                self.hooks.append(hook)
                ddawa_count += 1
                print(f"âœ… æ³¨å†Œé’©å­åˆ°DDAWAæ¨¡å—: {name}")

        # æŸ¥æ‰¾Detectå±‚
        detect_count = 0
        for name, module in self.model.named_modules():
            if 'Detect' in str(type(module)):
                hook = module.register_forward_hook(self._get_detect_hook(name))
                self.hooks.append(hook)
                detect_count += 1
                print(f"âœ… æ³¨å†Œé’©å­åˆ°Detectå±‚: {name}")

        # æŸ¥æ‰¾å…³é”®å·ç§¯å±‚
        conv_count = 0
        target_layers = ['10', '14', '18', '21']  # å…³é”®å·ç§¯å±‚
        for name, module in self.model.named_modules():
            if any(target in name for target in target_layers) and isinstance(module, torch.nn.Conv2d):
                hook = module.register_forward_hook(self._get_conv_hook(name))
                self.hooks.append(hook)
                conv_count += 1
                print(f"âœ… æ³¨å†Œé’©å­åˆ°å…³é”®å±‚: {name}")

        print(f"âœ… æ€»å…±æ³¨å†Œäº† {ddawa_count} ä¸ªDDAWAæ¨¡å—, {detect_count} ä¸ªDetectå±‚, {conv_count} ä¸ªå…³é”®å±‚")
        return True

    def _get_ddawa_hook(self, name):
        """åˆ›å»ºDDAWAæ¨¡å—çš„é’©å­å‡½æ•°"""

        def hook(module, input, output):
            try:
                # æ•è·æ³¨æ„åŠ›æƒé‡
                attention_data = {}

                # æ£€æŸ¥æ˜¯å¦æœ‰ddawa_moduleså±æ€§
                if hasattr(module, 'ddawa_modules'):
                    for i, ddawa_module in enumerate(module.ddawa_modules):
                        # æ•è·é€šé“æ³¨æ„åŠ›
                        if hasattr(ddawa_module, 'channel_attention'):
                            channel_att = ddawa_module.channel_attention
                            # å°è¯•è·å–æœ€ç»ˆçš„æ³¨æ„åŠ›æƒé‡
                            if hasattr(channel_att, 'weight'):
                                attention_data[f'channel_att_{i}'] = channel_att.weight.detach().cpu().numpy()

                        # æ•è·ç©ºé—´æ³¨æ„åŠ›
                        if hasattr(ddawa_module, 'spatial_attention'):
                            spatial_att = ddawa_module.spatial_attention
                            # è¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“å®ç°è·å–æƒé‡
                            attention_data[f'spatial_att_{i}'] = None

                if attention_data:
                    self.attention_maps[name] = attention_data

                # æ•è·è¾“å‡ºç‰¹å¾å›¾
                if isinstance(output, torch.Tensor):
                    self.feature_maps[name] = output.detach().cpu().numpy()

            except Exception as e:
                print(f"âŒ DDAWAé’©å­é”™è¯¯ ({name}): {e}")

        return hook

    def _get_detect_hook(self, name):
        """åˆ›å»ºDetectå±‚çš„é’©å­å‡½æ•°"""

        def hook(module, input, output):
            try:
                if isinstance(output, torch.Tensor):
                    self.feature_maps[name] = output.detach().cpu().numpy()
                # æ•è·æ£€æµ‹ç»“æœ
                self.detection_output = output
            except Exception as e:
                print(f"âŒ Detecté’©å­é”™è¯¯: {e}")

        return hook

    def _get_conv_hook(self, name):
        """åˆ›å»ºå·ç§¯å±‚çš„é’©å­å‡½æ•°"""

        def hook(module, input, output):
            try:
                if isinstance(output, torch.Tensor):
                    self.feature_maps[name] = output.detach().cpu().numpy()
            except Exception as e:
                print(f"âŒ Convé’©å­é”™è¯¯ ({name}): {e}")

        return hook

    def preprocess_image(self, image_path):
        """é¢„å¤„ç†å›¾åƒ"""
        # è¯»å–å›¾åƒ
        img = cv2.imread(image_path)
        if img is None:
            raise ValueError(f"æ— æ³•è¯»å–å›¾åƒ: {image_path}")

        # è½¬æ¢ä¸ºRGB
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        original_img = img_rgb.copy()

        # ä¿å­˜åŸå§‹å°ºå¯¸
        h, w = img_rgb.shape[:2]

        # è°ƒæ•´å¤§å°ï¼ˆä¿æŒå®½é«˜æ¯”ï¼‰
        scale = min(self.img_size / w, self.img_size / h)
        new_w, new_h = int(w * scale), int(h * scale)

        # è°ƒæ•´å›¾åƒå¤§å°
        resized_img = cv2.resize(img_rgb, (new_w, new_h))

        # åˆ›å»ºå¡«å……å›¾åƒ
        padded_img = np.zeros((self.img_size, self.img_size, 3), dtype=np.uint8)
        padded_img[:new_h, :new_w] = resized_img

        # è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼ï¼ˆç¡®ä¿æ•°æ®ç±»å‹åŒ¹é…ï¼‰
        input_tensor = torch.from_numpy(padded_img).float().permute(2, 0, 1).unsqueeze(0) / 255.0
        input_tensor = input_tensor.to(self.device)

        # ç¡®ä¿è¾“å…¥æ•°æ®ç±»å‹ä¸æ¨¡å‹ä¸€è‡´
        if next(self.model.parameters()).dtype == torch.float32:
            input_tensor = input_tensor.float()
        elif next(self.model.parameters()).dtype == torch.float16:
            input_tensor = input_tensor.half()

        return input_tensor, original_img, (w, h), scale

    def generate_heatmaps(self, image_path, output_dir="yolov8_ddawa_heatmaps"):
        """ç”Ÿæˆçƒ­åŠ›å›¾"""
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)

        # åŠ è½½æ¨¡å‹
        if not self.load_model():
            print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥")
            return

        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        self.model.eval()

        # æ³¨å†Œé’©å­
        if not self.register_hooks():
            print("âŒ é’©å­æ³¨å†Œå¤±è´¥")
            return

        # é¢„å¤„ç†å›¾åƒ
        try:
            input_tensor, original_img, original_size, scale = self.preprocess_image(image_path)
            print(f"âœ… å›¾åƒé¢„å¤„ç†å®Œæˆ: åŸå§‹å°ºå¯¸ {original_size}, ç¼©æ”¾æ¯”ä¾‹ {scale:.3f}")
            print(f"ğŸ“Š è¾“å…¥å¼ é‡å½¢çŠ¶: {input_tensor.shape}, æ•°æ®ç±»å‹: {input_tensor.dtype}")
            print(f"ğŸ“Š æ¨¡å‹å‚æ•°æ•°æ®ç±»å‹: {next(self.model.parameters()).dtype}")
        except Exception as e:
            print(f"âŒ å›¾åƒé¢„å¤„ç†å¤±è´¥: {e}")
            return

        # å‰å‘ä¼ æ’­
        print("ğŸš€ è¿›è¡Œå‰å‘ä¼ æ’­...")
        try:
            with torch.no_grad():
                # ç¡®ä¿è¾“å…¥å’Œæ¨¡å‹åœ¨åŒä¸€è®¾å¤‡ä¸Š
                input_tensor = input_tensor.to(self.device)
                if next(self.model.parameters()).dtype == torch.float16:
                    input_tensor = input_tensor.half()
                else:
                    input_tensor = input_tensor.float()

                output = self.model(input_tensor)
            print("âœ… å‰å‘ä¼ æ’­å®Œæˆ")
        except Exception as e:
            print(f"âŒ å‰å‘ä¼ æ’­å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return

        # ç§»é™¤é’©å­
        for hook in self.hooks:
            hook.remove()

        print(f"ğŸ“Š æ•è·åˆ° {len(self.feature_maps)} ä¸ªç‰¹å¾å›¾")
        print(f"ğŸ“Š æ•è·åˆ° {len(self.attention_maps)} ä¸ªæ³¨æ„åŠ›å›¾")

        # ä¿å­˜åŸå§‹å›¾åƒ
        plt.figure(figsize=(10, 8))
        plt.imshow(original_img)
        plt.title('Original Image', fontsize=16)
        plt.axis('off')
        plt.savefig(f'{output_dir}/original_image.jpg', dpi=300, bbox_inches='tight')
        plt.close()

        # å¯è§†åŒ–ç‰¹å¾çƒ­åŠ›å›¾
        self._visualize_feature_maps(original_img, output_dir)

        # å¯è§†åŒ–æ³¨æ„åŠ›çƒ­åŠ›å›¾
        if self.attention_maps:
            self._visualize_attention_maps(original_img, output_dir)

        print(f"âœ… çƒ­åŠ›å›¾ç”Ÿæˆå®Œæˆï¼ç»“æœä¿å­˜åœ¨: {output_dir}/")

    def _visualize_feature_maps(self, original_img, output_dir):
        """å¯è§†åŒ–ç‰¹å¾å›¾"""
        if not self.feature_maps:
            print("âš ï¸ æ²¡æœ‰æ•è·åˆ°ç‰¹å¾å›¾")
            return

        # é€‰æ‹©é‡è¦çš„å±‚è¿›è¡Œå¯è§†åŒ–
        important_layers = []
        for name in self.feature_maps.keys():
            # é€‰æ‹©åŒ…å«DDAWAã€Detectæˆ–å…³é”®æ•°å­—çš„å±‚
            if 'ConcatWithDDAWA' in name or 'Detect' in name or any(str(i) in name for i in [10, 12, 14, 16, 18, 21]):
                important_layers.append(name)

        if not important_layers:
            important_layers = list(self.feature_maps.keys())[:8]  # å–å‰8ä¸ª

        # åˆ›å»ºç‰¹å¾å›¾å¯è§†åŒ–
        n_cols = min(4, len(important_layers))
        n_rows = (len(important_layers) + n_cols - 1) // n_cols

        fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 5 * n_rows))

        if n_rows == 1 and n_cols == 1:
            axes = np.array([[axes]])
        elif n_rows == 1:
            axes = axes.reshape(1, -1)

        for i, layer_name in enumerate(important_layers):
            row = i // n_cols
            col = i % n_cols

            if row >= n_rows:
                break

            feature_map = self.feature_maps[layer_name]

            try:
                # å¤„ç†ä¸åŒå½¢çŠ¶çš„ç‰¹å¾å›¾
                if len(feature_map.shape) == 4:  # [batch, channels, height, width]
                    # å–ç¬¬ä¸€ä¸ªbatchå’Œæ‰€æœ‰é€šé“çš„å‡å€¼
                    heatmap = np.mean(feature_map[0], axis=0)
                elif len(feature_map.shape) == 3:  # [channels, height, width]
                    heatmap = np.mean(feature_map, axis=0)
                elif len(feature_map.shape) == 2:  # [height, width]
                    heatmap = feature_map
                else:
                    continue

                # è°ƒæ•´åˆ°åŸå§‹å›¾åƒå°ºå¯¸
                h, w = original_img.shape[:2]
                heatmap_resized = cv2.resize(heatmap, (w, h))

                # å½’ä¸€åŒ–
                heatmap_resized = (heatmap_resized - heatmap_resized.min()) / (
                            heatmap_resized.max() - heatmap_resized.min() + 1e-8)

                # æ˜¾ç¤ºçƒ­åŠ›å›¾
                axes[row, col].imshow(heatmap_resized, cmap='jet', alpha=0.7)
                axes[row, col].imshow(original_img, alpha=0.5)
                axes[row, col].set_title(f'{layer_name}\n{feature_map.shape}', fontsize=9)
                axes[row, col].axis('off')

            except Exception as e:
                print(f"âŒ å¤„ç†ç‰¹å¾å›¾ {layer_name} å¤±è´¥: {e}")
                axes[row, col].axis('off')

        # éšè—å¤šä½™çš„å­å›¾
        for i in range(len(important_layers), n_rows * n_cols):
            row = i // n_cols
            col = i % n_cols
            if row < n_rows and col < n_cols:
                axes[row, col].axis('off')

        plt.tight_layout()
        plt.savefig(f'{output_dir}/feature_heatmaps.jpg', dpi=300, bbox_inches='tight')
        plt.close()

        print(f"âœ… ç‰¹å¾çƒ­åŠ›å›¾å·²ä¿å­˜")

    def _visualize_attention_maps(self, original_img, output_dir):
        """å¯è§†åŒ–æ³¨æ„åŠ›å›¾"""
        if not self.attention_maps:
            print("âš ï¸ æ²¡æœ‰æ•è·åˆ°æ³¨æ„åŠ›å›¾")
            return

        for layer_name, attention_data in self.attention_maps.items():
            print(f"ğŸ“‹ å¤„ç†æ³¨æ„åŠ›å±‚: {layer_name}")

            # åˆ›å»ºæ³¨æ„åŠ›å¯è§†åŒ–
            fig, axes = plt.subplots(1, 2, figsize=(15, 6))

            # é€šé“æ³¨æ„åŠ›å¯è§†åŒ–
            if 'channel_att_0' in attention_data and attention_data['channel_att_0'] is not None:
                channel_att = attention_data['channel_att_0']
                axes[0].bar(range(len(channel_att)), channel_att)
                axes[0].set_title(f'{layer_name} - Channel Attention')
                axes[0].set_xlabel('Channel Index')
                axes[0].set_ylabel('Attention Weight')
                axes[0].grid(True, alpha=0.3)
            else:
                axes[0].text(0.5, 0.5, 'Channel Attention Data\nNot Available',
                             ha='center', va='center', fontsize=12)
                axes[0].set_title(f'{layer_name} - Channel Attention')
                axes[0].axis('off')

            # ç©ºé—´æ³¨æ„åŠ›å ä½ç¬¦
            axes[1].text(0.5, 0.5, 'Spatial Attention\n(éœ€è¦å…·ä½“å®ç°)',
                         ha='center', va='center', fontsize=12)
            axes[1].set_title(f'{layer_name} - Spatial Attention')
            axes[1].axis('off')

            plt.tight_layout()
            safe_name = layer_name.replace('.', '_').replace('(', '').replace(')', '')
            plt.savefig(f'{output_dir}/attention_{safe_name}.jpg', dpi=300, bbox_inches='tight')
            plt.close()

        print(f"âœ… æ³¨æ„åŠ›å›¾å·²ä¿å­˜")


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹YOLOv8-DDAWAçƒ­åŠ›å›¾åˆ†æ...")

    # é…ç½®å‚æ•°
    model_path = ".pt"  # æ‚¨çš„YOLOv8-DDAWAæ¨¡å‹æ–‡ä»¶
    image_path = "val1.jpg"  # æµ‹è¯•å›¾åƒ
    output_dir = "yolov8_ddawa_heatmaps_results"

    print(f"ğŸ“ æ¨¡å‹è·¯å¾„: {model_path}")
    print(f"ğŸ–¼ï¸  å›¾åƒè·¯å¾„: {image_path}")
    print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {output_dir}")

    # åˆ›å»ºå¯è§†åŒ–å™¨
    visualizer = YOLOv8DDAWAHeatmapVisualizer(model_path)

    # ç”Ÿæˆçƒ­åŠ›å›¾
    visualizer.generate_heatmaps(image_path, output_dir)


    print("ğŸ‰ åˆ†æå®Œæˆï¼")
